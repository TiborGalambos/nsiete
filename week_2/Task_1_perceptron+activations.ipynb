{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# In Today's task you will\n",
    "\n",
    "- Implement Linear (also called Dense, Fully-Connected) layer as a Perceptron.\n",
    "- Allow your solution to stack multiple layers to form MLP network.\n",
    "- Perform forward propagation through your network.\n",
    "\n",
    "This (and later) template implementation is similar to Pytorch framework."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 1a:\n",
    "\n",
    "Declare a simple perceptron (Linear layer) that inherits defined class Module - it is here, to help you store all network layers.\n",
    "\n",
    "The simple perceptron should be constructed of:\n",
    "1. Input features\n",
    "2. Followed by 1 Linear Layer with \"single neuron\"\n",
    "3. Activation function\n",
    "\n",
    "\n",
    "4. Perform forward pass for the example feature vectors `xInput1` and `xInput2` of `size = 10` features.\n",
    "Use prepared plot to view the results. (Repeat the process using all 4 activation functions.)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Import\n",
    "import numpy as np\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from collections import OrderedDict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Module\n",
    "\n",
    "All deep learning frameworks have usually one elementary building block.\n",
    "In our project, we follow the structure of the pytorch, so the elementary building block is called **`Module`**.\n",
    "Now, it is pretty simple, but it will get more complex and more useful...\n",
    "You can see function `.backward` that will later contain the partial derivations of chain rule for backward pass and parameter optimization."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "class Module:\n",
    "    def __init__(self):\n",
    "        self.modules = OrderedDict()\n",
    "\n",
    "    def add_module(self, module, name:str):\n",
    "        if hasattr(self, name) and name not in self.modules:\n",
    "            raise KeyError(\"attribute '{}' already exists\".format(name))\n",
    "        elif '.' in name:\n",
    "            raise KeyError(\"module name can't contain \\\".\\\"\")\n",
    "        elif name == '':\n",
    "            raise KeyError(\"module name can't be empty string \\\"\\\"\")\n",
    "        self.modules[name] = module\n",
    "\n",
    "    def forward(self, *args, **kwargs) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def backward(self, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Linear Layer\n",
    "\n",
    "In the lecture, we talked about a Perceptron and Single Layer Perceptron as an object with weight for every input value.\n",
    "In the frameworks, the \"Fully connected layer\" is implemented in Matrix Algebra.\n",
    "\n",
    "Also, the activation function and layer logic are separated for easier backward propagation (chain rule) and optimization (The topic of 2nd+3rd lecture).\n",
    "\n",
    "(If you want to know more, you can go to the lecture, or you can take a look on the implementation of forward and backward propagation on your own.)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "#   Linear class\n",
    "#------------------------------------------------------------------------------\n",
    "class Linear(Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(Linear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.W = np.random.randn(out_features, in_features)\n",
    "        self.b = np.zeros((out_features,))\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        # >>>>>>>>> add here\n",
    "        return\n",
    "        # <<<<<<<<<\n",
    "\n",
    "    def backward(self, dNet):\n",
    "        pass\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Activations\n",
    "\n",
    "The definitions for Sigmoid, Tanh, ReLU, and LeakyReLU activation functions with forward and backward pass.\n",
    "Implement the forward pass. (for now, you leave the backward pass on `pass`)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#------------------------------------------------------------------------------\n",
    "#   SigmoidActivationFunction class\n",
    "#------------------------------------------------------------------------------\n",
    "class Sigmoid(Module):\n",
    "    def __init__(self):\n",
    "        super(Sigmoid, self).__init__()\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        # >>>>>>>>> add here\n",
    "        return\n",
    "        # <<<<<<<<<\n",
    "\n",
    "    def backward(self, dNet):\n",
    "        pass\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#   HyperbolicTangentActivationFunction class\n",
    "#------------------------------------------------------------------------------\n",
    "class Tanh(Module):\n",
    "    def __init__(self):\n",
    "        super(Tanh, self).__init__()\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        # >>>>>>>>> add here\n",
    "        return\n",
    "        # <<<<<<<<<\n",
    "\n",
    "    def backward(self, dNet):\n",
    "        pass\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#   RELUActivationFunction class\n",
    "#------------------------------------------------------------------------------\n",
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "        super(ReLU, self).__init__()\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        # >>>>>>>>> add here\n",
    "        return\n",
    "        # <<<<<<<<<\n",
    "\n",
    "    def backward(self, dNet):\n",
    "        pass\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#   LeakyRELUActivationFunction class\n",
    "#------------------------------------------------------------------------------\n",
    "class LeakyReLU(Module):\n",
    "    # >>>>>>>>> add something here\n",
    "    def __init__(self):\n",
    "        super(LeakyReLU, self).__init__()\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        return\n",
    "    # <<<<<<<<<<<\n",
    "    def backward(self, dNet):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plotting the functions\n",
    "Verify your implementations of Activation functions - do your graphs look like they should?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "activationsInput = np.linspace(-4,4,100)\n",
    "\n",
    "sigmoid = Sigmoid()\n",
    "y = sigmoid.forward(activationsInput)\n",
    "\n",
    "fig = make_subplots(rows=2, cols=2)\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=activationsInput, y=y, name='Sigmoid'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "tanh = Tanh()\n",
    "y = tanh.forward(activationsInput)\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=activationsInput, y=y, name='Tanh'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "relu = ReLU()\n",
    "y = relu(activationsInput)\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=activationsInput, y=y, name='ReLU'),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "leakyrelu = LeakyReLU()\n",
    "y = leakyrelu(activationsInput)\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=activationsInput, y=y, name='LeakyReLU'),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=600, width=800, title_text=\"Activation functions\")\n",
    "fig.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Perceptron feed forward\n",
    "\n",
    "Model your Perceptron.\n",
    "Define and initialize perceptron with \"1 neuron\"!\n",
    "Feed `xInput1` and `xInput2` to the perceptron and print the results."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "xInput1 = np.arange(10)\n",
    "xInput2 = np.random.random(10)\n",
    "# >>>>>>>>> Initialize Your Perceptron Here\n",
    "# There are multiple possibilities, there is not the Chosen ONE!\n",
    "\n",
    "# <<<<<<<<< Use as many lines as you need"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Your Perceptron with an Activation function\n",
    "Use previously defined perceptron and use its output as input for the activation function sigmoid and LeakyReLU.\n",
    "Feed `xInput1` and `xInput2` to the perceptron, print and observe the results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# >>>>>>>>> Initialize activations and feed them after perceptron\n",
    "\n",
    "# <<<<<<<<< Use as many lines as you need"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 1b:\n",
    "\n",
    "Finish the implementation of class `Model` - finish the call of forward feed.\n",
    "Declare a simple model consisting of:\n",
    " 1. Input Layer\n",
    " 2. 3 Linear Layers with arbitrary number of neurons\n",
    " 3. Output Linear Layer with 1 neuron.\n",
    "\n",
    "...and activation functions to add non-linearity\n",
    "\n",
    "Declare your own input vector with 16 features.\n",
    "Perform forward pass through the network and print the results.\n",
    "\n",
    "### Model class\n",
    "\n",
    "Implementation of the **`Model`** class.\n",
    "Define its forward function - the implementation of forward and backward pass is sensitive to the order of called operations.\n",
    "Each Layer(module) of type **`Module`** can be saved to the attribute **`Module.modules`** using the **`add_module`** method.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#------------------------------------------------------------------------------\n",
    "#   Model class\n",
    "#------------------------------------------------------------------------------\n",
    "class Model(Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        # >>>>>>>>> add here\n",
    "        return\n",
    "        # <<<<<<<< do something beautiful... and simple\n",
    "\n",
    "    def backward(self, dA: np.ndarray):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 50,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "model = Model()\n",
    "# >>>>>>>>> Build the model architecture with 3 layers (input, hidden, output) that can process - feed forward the xInput1 and xInput2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print(model. ...)\n",
    "# print(model. ...)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}