{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Generovanie aut"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Set device to use CUDA if available, otherwise use CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "path = 'C:/Users/tibor/PycharmProjects/nsiete3/data/dataset'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "import os\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "def printTable():\n",
    "    def count_files(path):\n",
    "        num_files = 0\n",
    "        for root, dirs, files in os.walk(path):\n",
    "            for file in files:\n",
    "                num_files += 1\n",
    "        return num_files\n",
    "\n",
    "    directories = [dir for dir in os.listdir(path)\n",
    "                   if os.path.isdir(os.path.join(path, dir))]\n",
    "\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"folder\", \"img count\"]\n",
    "\n",
    "    for dir in directories:\n",
    "        dir_path = os.path.join(path, dir)\n",
    "        file_count = count_files(dir_path)\n",
    "        table.add_row([dir, file_count])\n",
    "\n",
    "    table.sortby = \"img count\"\n",
    "\n",
    "    print(table)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+\n",
      "|    folder    | img count |\n",
      "+--------------+-----------+\n",
      "|    truck     |    396    |\n",
      "| personal_car |    8792   |\n",
      "+--------------+-----------+\n"
     ]
    }
   ],
   "source": [
    "printTable()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "data_dir = path\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "dataset = datasets.ImageFolder(root=path, transform=transform)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=False, num_workers=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "mean = 0.\n",
    "std = 0.\n",
    "total_images = 0\n",
    "for images, _ in data_loader:\n",
    "    batch_samples = images.size(0)\n",
    "    images = images.view(batch_samples, images.size(1), -1)\n",
    "    mean += images.mean(2).sum(0)\n",
    "    std += images.std(2).sum(0)\n",
    "    total_images += batch_samples\n",
    "\n",
    "mean /= total_images\n",
    "std /= total_images\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  tensor([0.2946, 0.3051, 0.3091])\n",
      "Std:  tensor([0.2032, 0.1997, 0.1998])\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean: \", mean)\n",
    "print(\"Std: \", std)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Rerun from here"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "path = 'C:/Users/tibor/PycharmProjects/nsiete3/data/dataset'\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize(mean=[0.2946, 0.3051, 0.3091], std=[0.2032, 0.1997, 0.1998])\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.ImageFolder(root=path, transform=transform)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_size = int(0.99 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)\n",
    "        self.fc1 = nn.Linear(256 * 4 * 4, 512)\n",
    "        self.fc21 = nn.Linear(512, latent_size)\n",
    "        self.fc22 = nn.Linear(512, latent_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = x.view(-1, 256 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        mu = self.fc21(x)\n",
    "        logvar = self.fc22(x)\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps*std\n",
    "        return z, mu, logvar\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_size):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(latent_size, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 4 * 4 * 128)\n",
    "        self.conv1 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv3 = nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv4 = nn.ConvTranspose2d(16, 3, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = F.relu(self.fc1(z))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = x.view(-1, 128, 4, 4)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = torch.sigmoid(self.conv4(x))\n",
    "        return x\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_size):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_size = latent_size\n",
    "        self.encoder = Encoder(latent_size)\n",
    "        self.decoder = Decoder(latent_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, mu, logvar = self.encoder(x)\n",
    "        recon_x = self.decoder(z)\n",
    "        return recon_x, mu, logvar\n",
    "\n",
    "    def sample(self, num_samples):\n",
    "        with torch.no_grad():\n",
    "            z = torch.randn(num_samples, self.latent_size).to(device)\n",
    "            samples = self.decoder(z).cpu()\n",
    "            return samples"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Finishing last run (ID:663oq1pm) before initializing another..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.071 MB of 0.071 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d3342d73878242a788633487af11c3c9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>vae_loss</td><td>█▆▇▇▇▆▆▆▄▆▅▅▃▅▅▃▃▅▃▃▄▃▁▅▅▅▃▄▄▄▄▄▅▅▅▅▅▅▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>vae_loss</td><td>118408.72656</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">fallen-dragon-12</strong> at: <a href='https://wandb.ai/neuronkytim/car-generation/runs/663oq1pm' target=\"_blank\">https://wandb.ai/neuronkytim/car-generation/runs/663oq1pm</a><br/>Synced 5 W&B file(s), 10 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20230419_210627-663oq1pm\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Successfully finished last run (ID:663oq1pm). Initializing new run:<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333338766, max=1.0)…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "00f35a023ab748798a4aad5ebac64c99"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.14.2"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\Users\\tibor\\PycharmProjects\\nsiete3\\wandb\\run-20230419_210757-g5gfor4i</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/neuronkytim/car-generation/runs/g5gfor4i' target=\"_blank\">pleasant-capybara-13</a></strong> to <a href='https://wandb.ai/neuronkytim/car-generation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/neuronkytim/car-generation' target=\"_blank\">https://wandb.ai/neuronkytim/car-generation</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/neuronkytim/car-generation/runs/g5gfor4i' target=\"_blank\">https://wandb.ai/neuronkytim/car-generation/runs/g5gfor4i</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 529/529 [00:51<00:00, 10.29it/s]\n",
      "Epoch 2/10: 100%|██████████| 529/529 [00:50<00:00, 10.43it/s]\n",
      "Epoch 3/10: 100%|██████████| 529/529 [00:52<00:00, 10.08it/s]\n",
      "Epoch 4/10: 100%|██████████| 529/529 [00:50<00:00, 10.55it/s]\n",
      "Epoch 5/10: 100%|██████████| 529/529 [00:49<00:00, 10.63it/s]\n",
      "Epoch 6/10: 100%|██████████| 529/529 [00:49<00:00, 10.60it/s]\n",
      "Epoch 7/10: 100%|██████████| 529/529 [00:51<00:00, 10.24it/s]\n",
      "Epoch 8/10: 100%|██████████| 529/529 [00:49<00:00, 10.58it/s]\n",
      "Epoch 9/10: 100%|██████████| 529/529 [00:50<00:00, 10.40it/s]\n",
      "Epoch 10/10: 100%|██████████| 529/529 [00:51<00:00, 10.35it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "\n",
    "usewandb = True\n",
    "\n",
    "# Define hyperparameters\n",
    "batch_size = 16\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "latent_size = 256\n",
    "\n",
    "# Create the VAE model and optimizer\n",
    "vae = VAE(latent_size).to(device)\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define the loss function\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "# Initialize WandB\n",
    "if usewandb:\n",
    "    wandb.init(project=\"car-generation\")\n",
    "\n",
    "    # Log hyperparameters\n",
    "    config = wandb.config\n",
    "    config.batch_size = batch_size\n",
    "    config.learning_rate = learning_rate\n",
    "    config.num_epochs = num_epochs\n",
    "    config.latent_size = latent_size\n",
    "\n",
    "# Train the VAE\n",
    "vae.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, _) in enumerate(tqdm(trainloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
    "        images = images.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_images, mu, logvar = vae(images)\n",
    "        loss = loss_function(recon_images, images, mu, logvar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if usewandb:\n",
    "            # Log loss to WandB\n",
    "            wandb.log({\"vae_loss\": loss})\n",
    "\n",
    "    # Generate new car images\n",
    "    vae.eval()\n",
    "    with torch.no_grad():\n",
    "        num_samples = 10\n",
    "        samples = vae.sample(num_samples)\n",
    "        torchvision.utils.save_image(samples, f\"generated_cars{epoch}.png\")\n",
    "        if usewandb:\n",
    "            # Log generated images to WandB\n",
    "            wandb.log({\"generated_images\": [wandb.Image(sample) for sample in samples]})\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Next training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "torch.save(vae, f\"vae_model_epoch_{10}.pth\")\n",
    "torch.save(optimizer.state_dict(), f\"optimizer_epoch_{10}.pth\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|██████████| 529/529 [00:46<00:00, 11.26it/s]\n",
      "Epoch 2/100: 100%|██████████| 529/529 [00:45<00:00, 11.52it/s]\n",
      "Epoch 3/100: 100%|██████████| 529/529 [00:45<00:00, 11.59it/s]\n",
      "Epoch 4/100: 100%|██████████| 529/529 [00:45<00:00, 11.55it/s]\n",
      "Epoch 5/100: 100%|██████████| 529/529 [00:44<00:00, 11.79it/s]\n",
      "Epoch 6/100: 100%|██████████| 529/529 [00:46<00:00, 11.46it/s]\n",
      "Epoch 7/100: 100%|██████████| 529/529 [00:45<00:00, 11.74it/s]\n",
      "Epoch 8/100: 100%|██████████| 529/529 [00:44<00:00, 11.77it/s]\n",
      "Epoch 9/100: 100%|██████████| 529/529 [00:45<00:00, 11.55it/s]\n",
      "Epoch 10/100: 100%|██████████| 529/529 [00:46<00:00, 11.42it/s]\n",
      "Epoch 11/100: 100%|██████████| 529/529 [00:44<00:00, 11.84it/s]\n",
      "Epoch 12/100: 100%|██████████| 529/529 [00:44<00:00, 11.79it/s]\n",
      "Epoch 13/100: 100%|██████████| 529/529 [00:45<00:00, 11.57it/s]\n",
      "Epoch 14/100: 100%|██████████| 529/529 [00:45<00:00, 11.56it/s]\n",
      "Epoch 15/100: 100%|██████████| 529/529 [00:45<00:00, 11.60it/s]\n",
      "Epoch 16/100: 100%|██████████| 529/529 [00:46<00:00, 11.26it/s]\n",
      "Epoch 17/100: 100%|██████████| 529/529 [00:46<00:00, 11.28it/s]\n",
      "Epoch 18/100: 100%|██████████| 529/529 [00:46<00:00, 11.43it/s]\n",
      "Epoch 19/100: 100%|██████████| 529/529 [00:44<00:00, 11.79it/s]\n",
      "Epoch 20/100: 100%|██████████| 529/529 [00:46<00:00, 11.48it/s]\n",
      "Epoch 21/100: 100%|██████████| 529/529 [00:46<00:00, 11.28it/s]\n",
      "Epoch 22/100: 100%|██████████| 529/529 [00:46<00:00, 11.45it/s]\n",
      "Epoch 23/100: 100%|██████████| 529/529 [00:45<00:00, 11.58it/s]\n",
      "Epoch 24/100: 100%|██████████| 529/529 [00:45<00:00, 11.60it/s]\n",
      "Epoch 25/100: 100%|██████████| 529/529 [00:45<00:00, 11.58it/s]\n",
      "Epoch 26/100: 100%|██████████| 529/529 [00:45<00:00, 11.59it/s]\n",
      "Epoch 27/100: 100%|██████████| 529/529 [00:46<00:00, 11.39it/s]\n",
      "Epoch 28/100: 100%|██████████| 529/529 [00:46<00:00, 11.39it/s]\n",
      "Epoch 29/100: 100%|██████████| 529/529 [00:45<00:00, 11.53it/s]\n",
      "Epoch 30/100: 100%|██████████| 529/529 [00:45<00:00, 11.65it/s]\n",
      "Epoch 31/100: 100%|██████████| 529/529 [00:46<00:00, 11.35it/s]\n",
      "Epoch 32/100: 100%|██████████| 529/529 [00:46<00:00, 11.50it/s]\n",
      "Epoch 33/100: 100%|██████████| 529/529 [00:46<00:00, 11.27it/s]\n",
      "Epoch 34/100: 100%|██████████| 529/529 [00:47<00:00, 11.25it/s]\n",
      "Epoch 35/100: 100%|██████████| 529/529 [00:47<00:00, 11.20it/s]\n",
      "Epoch 36/100: 100%|██████████| 529/529 [00:47<00:00, 11.10it/s]\n",
      "Epoch 37/100: 100%|██████████| 529/529 [00:47<00:00, 11.24it/s]\n",
      "Epoch 38/100: 100%|██████████| 529/529 [00:46<00:00, 11.27it/s]\n",
      "Epoch 39/100: 100%|██████████| 529/529 [00:46<00:00, 11.47it/s]\n",
      "Epoch 40/100: 100%|██████████| 529/529 [00:47<00:00, 11.25it/s]\n",
      "Epoch 41/100: 100%|██████████| 529/529 [00:46<00:00, 11.36it/s]\n",
      "Epoch 42/100: 100%|██████████| 529/529 [00:46<00:00, 11.34it/s]\n",
      "Epoch 43/100: 100%|██████████| 529/529 [00:46<00:00, 11.47it/s]\n",
      "Epoch 44/100: 100%|██████████| 529/529 [00:44<00:00, 11.78it/s]\n",
      "Epoch 45/100: 100%|██████████| 529/529 [00:44<00:00, 11.81it/s]\n",
      "Epoch 46/100: 100%|██████████| 529/529 [00:44<00:00, 11.84it/s]\n",
      "Epoch 47/100: 100%|██████████| 529/529 [00:44<00:00, 11.81it/s]\n",
      "Epoch 48/100: 100%|██████████| 529/529 [00:44<00:00, 11.82it/s]\n",
      "Epoch 49/100: 100%|██████████| 529/529 [00:44<00:00, 11.76it/s]\n",
      "Epoch 50/100: 100%|██████████| 529/529 [00:44<00:00, 11.79it/s]\n",
      "Epoch 51/100: 100%|██████████| 529/529 [00:44<00:00, 11.87it/s]\n",
      "Epoch 52/100: 100%|██████████| 529/529 [00:44<00:00, 11.87it/s]\n",
      "Epoch 53/100: 100%|██████████| 529/529 [00:44<00:00, 11.85it/s]\n",
      "Epoch 54/100: 100%|██████████| 529/529 [00:44<00:00, 11.87it/s]\n",
      "Epoch 55/100: 100%|██████████| 529/529 [00:44<00:00, 11.88it/s]\n",
      "Epoch 56/100: 100%|██████████| 529/529 [00:44<00:00, 11.81it/s]\n",
      "Epoch 57/100: 100%|██████████| 529/529 [00:44<00:00, 11.86it/s]\n",
      "Epoch 58/100: 100%|██████████| 529/529 [00:44<00:00, 11.88it/s]\n",
      "Epoch 59/100: 100%|██████████| 529/529 [00:44<00:00, 11.87it/s]\n",
      "Epoch 60/100: 100%|██████████| 529/529 [00:44<00:00, 11.87it/s]\n",
      "Epoch 61/100: 100%|██████████| 529/529 [00:44<00:00, 11.86it/s]\n",
      "Epoch 62/100: 100%|██████████| 529/529 [00:46<00:00, 11.37it/s]\n",
      "Epoch 63/100:  63%|██████▎   | 334/529 [00:30<00:17, 11.08it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[44], line 32\u001B[0m\n\u001B[0;32m     30\u001B[0m vae\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[0;32m     31\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_epochs):\n\u001B[1;32m---> 32\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, (images, _) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(tqdm(trainloader, desc\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_epochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)):\n\u001B[0;32m     33\u001B[0m         images \u001B[38;5;241m=\u001B[39m images\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     34\u001B[0m         optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[1;32mc:\\users\\tibor\\pycharmprojects\\nsiete3\\venv\\lib\\site-packages\\tqdm\\std.py:1178\u001B[0m, in \u001B[0;36mtqdm.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1175\u001B[0m time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_time\n\u001B[0;32m   1177\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1178\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m iterable:\n\u001B[0;32m   1179\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[0;32m   1180\u001B[0m         \u001B[38;5;66;03m# Update and possibly print the progressbar.\u001B[39;00m\n\u001B[0;32m   1181\u001B[0m         \u001B[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;00m\n",
      "File \u001B[1;32mc:\\users\\tibor\\pycharmprojects\\nsiete3\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    625\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    626\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    627\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 628\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    629\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    631\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    632\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32mc:\\users\\tibor\\pycharmprojects\\nsiete3\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    669\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    670\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 671\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    672\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    673\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32mc:\\users\\tibor\\pycharmprojects\\nsiete3\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     56\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     57\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 58\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32mc:\\users\\tibor\\pycharmprojects\\nsiete3\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     56\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     57\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 58\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32mc:\\users\\tibor\\pycharmprojects\\nsiete3\\venv\\lib\\site-packages\\torch\\utils\\data\\dataset.py:295\u001B[0m, in \u001B[0;36mSubset.__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m    293\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(idx, \u001B[38;5;28mlist\u001B[39m):\n\u001B[0;32m    294\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindices[i] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m idx]]\n\u001B[1;32m--> 295\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindices\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\n",
      "File \u001B[1;32mc:\\users\\tibor\\pycharmprojects\\nsiete3\\venv\\lib\\site-packages\\torchvision\\datasets\\folder.py:229\u001B[0m, in \u001B[0;36mDatasetFolder.__getitem__\u001B[1;34m(self, index)\u001B[0m\n\u001B[0;32m    221\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    222\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m    223\u001B[0m \u001B[38;5;124;03m    index (int): Index\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    226\u001B[0m \u001B[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001B[39;00m\n\u001B[0;32m    227\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    228\u001B[0m path, target \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msamples[index]\n\u001B[1;32m--> 229\u001B[0m sample \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    230\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    231\u001B[0m     sample \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform(sample)\n",
      "File \u001B[1;32mc:\\users\\tibor\\pycharmprojects\\nsiete3\\venv\\lib\\site-packages\\torchvision\\datasets\\folder.py:268\u001B[0m, in \u001B[0;36mdefault_loader\u001B[1;34m(path)\u001B[0m\n\u001B[0;32m    266\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m accimage_loader(path)\n\u001B[0;32m    267\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 268\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mpil_loader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mc:\\users\\tibor\\pycharmprojects\\nsiete3\\venv\\lib\\site-packages\\torchvision\\datasets\\folder.py:248\u001B[0m, in \u001B[0;36mpil_loader\u001B[1;34m(path)\u001B[0m\n\u001B[0;32m    246\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(path, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[0;32m    247\u001B[0m     img \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mopen(f)\n\u001B[1;32m--> 248\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mimg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mRGB\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mc:\\users\\tibor\\pycharmprojects\\nsiete3\\venv\\lib\\site-packages\\PIL\\Image.py:933\u001B[0m, in \u001B[0;36mImage.convert\u001B[1;34m(self, mode, matrix, dither, palette, colors)\u001B[0m\n\u001B[0;32m    885\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconvert\u001B[39m(\n\u001B[0;32m    886\u001B[0m     \u001B[38;5;28mself\u001B[39m, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, matrix\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, dither\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, palette\u001B[38;5;241m=\u001B[39mPalette\u001B[38;5;241m.\u001B[39mWEB, colors\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m256\u001B[39m\n\u001B[0;32m    887\u001B[0m ):\n\u001B[0;32m    888\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    889\u001B[0m \u001B[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001B[39;00m\n\u001B[0;32m    890\u001B[0m \u001B[38;5;124;03m    method translates pixels through the palette.  If mode is\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    930\u001B[0m \u001B[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001B[39;00m\n\u001B[0;32m    931\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 933\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    935\u001B[0m     has_transparency \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtransparency\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    936\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m mode \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mP\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    937\u001B[0m         \u001B[38;5;66;03m# determine default mode\u001B[39;00m\n",
      "File \u001B[1;32mc:\\users\\tibor\\pycharmprojects\\nsiete3\\venv\\lib\\site-packages\\PIL\\ImageFile.py:269\u001B[0m, in \u001B[0;36mImageFile.load\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    266\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(msg)\n\u001B[0;32m    268\u001B[0m b \u001B[38;5;241m=\u001B[39m b \u001B[38;5;241m+\u001B[39m s\n\u001B[1;32m--> 269\u001B[0m n, err_code \u001B[38;5;241m=\u001B[39m \u001B[43mdecoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    270\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    271\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define hyperparameters\n",
    "batch_size = 16\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "latent_size = 256\n",
    "\n",
    "# Load the existing VAE model checkpoint\n",
    "vae = torch.load(\"models2/vae_model_epoch_20.pth\")\n",
    "\n",
    "# Load the optimizer state from the previous training run\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=learning_rate)\n",
    "optimizer.load_state_dict(torch.load(\"models2/optimizer_epoch_20.pth\"))\n",
    "\n",
    "# Define the loss function\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "# Train the VAE\n",
    "vae.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, _) in enumerate(tqdm(trainloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
    "        images = images.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_images, mu, logvar = vae(images)\n",
    "        loss = loss_function(recon_images, images, mu, logvar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Save the model checkpoint and optimizer state after each epoch\n",
    "    torch.save(vae, f\"vae_model_epoch_{epoch+1}.pth\")\n",
    "    torch.save(optimizer.state_dict(), f\"optimizer_epoch_{epoch+1}.pth\")\n",
    "\n",
    "\n",
    "\n",
    "    # Generate new car images\n",
    "    vae.eval()\n",
    "    with torch.no_grad():\n",
    "        num_samples = 16\n",
    "        samples = vae.sample(num_samples)\n",
    "        now = datetime.datetime.now()\n",
    "        timestamp = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        torchvision.utils.save_image(samples, f\"generated_cars{timestamp}.png\")\n",
    "        if usewandb:\n",
    "            # Log generated images to WandB\n",
    "            wandb.log({\"generated_images\": [wandb.Image(sample) for sample in samples]})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # Generate a random latent vector\n",
    "# z = torch.randn(1, 256)\n",
    "#\n",
    "# # Generate a new image from the latent vector\n",
    "# with torch.no_grad():\n",
    "#     generated_img = model.decode(z)\n",
    "#\n",
    "# # Display the generated image\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.imshow(generated_img[0].permute(1, 2, 0))\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "\n",
    "# Generate new car images\n",
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    num_samples = 10\n",
    "    samples = vae.sample(num_samples)\n",
    "    torchvision.utils.save_image(samples, f\"generated_cars.png{}\")\n",
    "    if usewandb:\n",
    "        # Log generated images to WandB\n",
    "        wandb.log({\"generated_images\": [wandb.Image(sample) for sample in samples]})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}